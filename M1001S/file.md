# Tecnol√≥gico de Monterrey  
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/Logo_del_ITESM.svg/1200px-Logo_del_ITESM.svg.png" alt="Logo Tec" width="100cm" height="100cm" /> 

**Desarrollo de Aplicaciones Avanzadas de Ciencias Computacionales**  

## Examen de Inteligencia Artificial

## Datos del Estudiante
**Nombre:** _________________________________________________  
**Matr√≠cula:** _________________  
**Fecha:** ___________________  

## Instrucciones
- ‚è±Ô∏è **Duraci√≥n:** 30 minutos
- ‚úèÔ∏è **Total preguntas:** 10
- ‚úÖ **Tipo:** Opci√≥n m√∫ltiple (solo una respuesta correcta por pregunta)
- üìù **Material permitido:** Ninguno
- üìå **Valor por pregunta:** 10 puntos

---

### 1. ¬øCu√°l es la funci√≥n de activaci√≥n m√°s adecuada para problemas de clasificaci√≥n binaria?
- a) ReLU  
- b) Tanh  
- c) Sigmoide



---

### 2. Para prevenir overfitting en un modelo, ¬øqu√© t√©cnica es m√°s efectiva?
a) Aumentar par√°metros  
b) Regularizaci√≥n (L1/L2)
d) Eliminar capas ocultas

---

### 3. Al trabajar con im√°genes, ¬øqu√© arquitectura es preferible?
a) RNN  
b) CNN
c) SVM  
d) √Årboles de decisi√≥n

---

### 4. ¬øQu√© algoritmo ajusta los pesos en redes neuronales?
a) Forward pass  
b) Gradiente descendente  
c) Regla de la cadena 
d) Todos los anteriores

---

### 5. Predecir el precio de una vivienda es un problema de:
a) Clasificaci√≥n  
b) Regresi√≥n 
c) Clustering  
d) Detecci√≥n de anomal√≠as

---

### 6. En CNNs, ¬øqu√© capa reduce dimensiones espaciales?
a) Convolucional  
b) Pooling 
c) Dropout  
d) BatchNorm

---

### 7. Con datos desbalanceados (95%-5%), ¬øqu√© m√©trica evitar?
a) Recall  
b) F1-score  
c) Accuracy  
d) AUC-ROC

---

### 8. ¬øC√≥mo se llama reutilizar modelos preentrenados?
a) Backpropagation  
b) Transfer Learning  
c) Data Augmentation  
d) Ensemble Learning

---

### 9. ¬øQu√© red solucion√≥ el vanishing gradient con conexiones residuales?
a) VGG16  
b) ResNet  
c) AlexNet  
d) Inception

---

### 10. Para encontrar patrones en datos sin etiquetas usamos:
a) Supervisado  
b) No supervisado  
c) Por refuerzo  
d) Semi-supervisado

---

### 11. Ejercicio Pr√°ctico: C√°lculo de Salida en Red Neuronal
Considere la siguiente red neuronal densa:
- **Capa de entrada:** 2 neuronas (x‚ÇÅ, x‚ÇÇ)
- **Capa oculta:** 3 neuronas (a‚ÇÅ, a‚ÇÇ, a‚ÇÉ) con funci√≥n de activaci√≥n ReLU
- **Capa de salida:** 1 neurona (y) con funci√≥n de activaci√≥n sigmoide

Pesos capa oculta (W¬π): 

|0.5 |-0.2| 
|-|-|
|0.3|0.8|           
|-0.6|1.0|  

Bias capa oculta (b¬π): 

| 0.1  | -0.3  | 0.5  |
|---|---|---|

Pesos capa salida (W¬≤): 

| 1.2  | -0.7  | 0.9  |
|---|---|---|

Bias capa salida (b¬≤): 

| -0.4  |
|---|

```mermaid
graph LR
    x1((x‚ÇÅ)) --> a1((a‚ÇÅ))
    x1 --> a2((a‚ÇÇ))
    x1 --> a3((a‚ÇÉ))
    x2((x‚ÇÇ)) --> a1
    x2 --> a2
    x2 --> a3
    a1 --> y((y))
    a2 --> y
    a3 --> y
    
    style x1 fill:#f9f,stroke:#333
    style x2 fill:#f9f,stroke:#333
    style a1 fill:#9f9,stroke:#333
    style a2 fill:#9f9,stroke:#333
    style a3 fill:#9f9,stroke:#333
    style y fill:#ff9,stroke:#333
```

---
