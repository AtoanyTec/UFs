# üß†ü§ñ Pr√°ctica 2: Seguimiento de Persona con YOLOv8-Seg

## üéØ Objetivo
Implementar **YOLOv8 para segmentaci√≥n** en la **Raspberry Pi** del robot m√≥vil para **detectar una persona** y, al localizarla, **mover el robot hacia ella** usando **cinem√°tica** (controlando **velocidad lineal `v`** y **velocidad angular `œâ`**).  
El giro debe basarse en el **√°ngulo requerido** para alinear al robot con la persona.

---

## ‚úÖ Requisitos
### Hardware
- Robot m√≥vil (diferencial u orugas) con **Raspberry Pi**
- **C√°mara** (USB o CSI) montada al frente del robot
- Sistema de control de motores operativo (driver/microcontrolador/ROS)
- Medici√≥n de par√°metros f√≠sicos del robot:
  - **Separaci√≥n entre ruedas/orugas**: $b (m)$
  - **Radio de rueda** (si aplica): $r (m)$

### Software
- Sistema operativo en Raspberry (Raspberry Pi OS o Ubuntu)
- Entorno Python + librer√≠as para:
  - Captura de video
  - YOLOv8 (Ultralytics) para segmentaci√≥n
  - Control del robot (GPIO/serial/ROS)

---

## üß© Parte A ‚Äî Verificaci√≥n del sistema base (antes de YOLO)
1. **Encender robot** y confirmar:
   - La Raspberry inicia sin errores.
   - La c√°mara es reconocida y entrega video estable.
2. **Verificar control de motores**:
   - El robot debe poder:
     - Avanzar
     - Girar a la izquierda
     - Girar a la derecha
     - Detenerse
3. Registrar en el reporte:
   - $b$ (y $r$ si aplica)
   - C√≥mo se env√≠an comandos a motores (ej. PWM, ROS `/cmd_vel`, microcontrolador por serial)

‚úÖ **Evidencia**: video corto de 20‚Äì30 s mostrando c√°mara + movimientos b√°sicos.

---

## üß† Parte B ‚Äî Detecci√≥n por segmentaci√≥n (YOLOv8-Seg)
### 1) Selecci√≥n del modelo
- Usar un modelo de segmentaci√≥n **ligero** para Raspberry (recomendaci√≥n: versi√≥n ‚Äúnano‚Äù o equivalente).
- Justificar la elecci√≥n considerando **FPS** y estabilidad.

### 2) Configurar detecci√≥n SOLO de persona
Tu pipeline debe:
- Procesar cada frame de la c√°mara
- Ejecutar segmentaci√≥n
- Filtrar resultados para la clase **person**
- Aplicar un umbral de confianza (ej. `conf ‚â• 0.5`, ajustable)

### 3) Extraer mediciones m√≠nimas por frame
Para la persona detectada (idealmente la de mayor confianza):
- **Centroide** `(cx, cy)` de la detecci√≥n (puede ser del bounding box o de la m√°scara)
- **Ancho de imagen** `W` (pixeles)
- **√Årea** aproximada de la m√°scara o caja (para estimar cercan√≠a)

‚úÖ **Evidencia**: captura/overlay o screenshot donde se aprecie:
- m√°scara o contorno
- caja de detecci√≥n
- etiqueta ‚Äúperson‚Äù
- centroide (o punto medio)

---

## üß≠ Parte C ‚Äî De imagen a √°ngulo (para girar ‚Äúlos grados correspondientes‚Äù)

### 1) Error horizontal (desalineaci√≥n)
Define:
- `W`: ancho de imagen
- `cx`: coordenada x del centroide de la persona
- Centro de imagen: $W/2$`

Error en pixeles:
- $e_x = cx - (W/2)$

Error normalizado:
- $e = e_x / (W/2)$  ‚Üí rango aproximado `[-1, 1]`

Interpretaci√≥n:
- `e < 0`: persona a la izquierda
- `e > 0`: persona a la derecha
- `e ‚âà 0`: persona centrada

### 2) Convertir error a √°ngulo (grados o radianes)
Necesitas el **FOV horizontal** de la c√°mara (Field of View), aproximado o medido.
- Den√≥talo como $FOV_h$ (en grados)

√Ångulo estimado hacia la persona:
- $Œ∏_target ‚âà e * (FOV_h / 2)$

‚úÖ Esto te da ‚Äúlos grados correspondientes‚Äù que el robot debe girar para apuntar a la persona.

üìå **En el reporte**:
- Indica de d√≥nde obtuviste $FOV_h$ (datasheet, medici√≥n aproximada, o calibraci√≥n).

---

## üîÅ Parte D ‚Äî Control cinem√°tico para orientar y avanzar
Tu robot debe usar **cinem√°tica** para convertir la percepci√≥n en movimiento.

### 1) Velocidad angular `œâ` (orientaci√≥n)
Plantea una ley de control proporcional:
- $œâ = k_œâ * Œ∏_target$

Con:
- saturaci√≥n: $|œâ| ‚â§ œâ_max$
- regla de seguridad: si `|Œ∏_target|` es grande, prioriza girar antes de avanzar.

üìå Recomendaci√≥n operativa:
- Si $|Œ∏_target| > Œ∏_align$ (ej. 10¬∞‚Äì15¬∞):  
  - **v = 0** y solo giras con `œâ`
- Si $|Œ∏_target| ‚â§ Œ∏_align$:  
  - permites avance (v>0)

### 2) Velocidad lineal `v` (avance)
Define una condici√≥n de ‚Äúcercan√≠a‚Äù usando el tama√±o del objeto:
- √°rea de m√°scara o √°rea de bounding box: `A`

Regla:
- Si `A < A_target`: la persona est√° lejos ‚Üí **avanza**
- Si `A ‚â• A_target`: persona suficientemente cerca ‚Üí **detente**

‚úÖ Esto evita choque y genera comportamiento de ‚Äúacercamiento‚Äù.

---

## üöó Parte E ‚Äî Conversi√≥n de (v, œâ) a velocidades de rueda/oruga (cinem√°tica)
### Robot diferencial / orugas (modelo diferencial)
Con separaci√≥n `b`:
- $v_R = v + (b/2) * œâ$
- $v_L = v - (b/2) * œâ$

En el reporte, explica:
- c√≥mo conviertes $v_L$ y $v_R$ a tu forma de control (PWM, RPM, comando ROS, etc.)
- l√≠mites y saturaci√≥n para no exigir m√°s de lo que los motores pueden dar

‚úÖ **Evidencia**: tabla o ejemplos num√©ricos con un caso:
- persona a la derecha (Œ∏_target positivo)
- calcula `œâ`
- obtiene $v_R, v_L$
- explica c√≥mo se traduce a se√±al de motor

---

## üß† Parte F ‚Äî M√°quina de estados (OBLIGATORIA)
Implementar al menos estos estados:

### Estado 0: BUSCAR
Condici√≥n: no hay persona confiable detectada.
Acci√≥n:
- `v = 0`
- `œâ = œâ_search` (giro lento constante) o barrido por intervalos
Transici√≥n:
- a SEGUIR cuando detectes ‚Äúperson‚Äù por N frames consecutivos (para evitar falsos positivos)

### Estado 1: ALINEAR
Condici√≥n: persona detectada pero $|Œ∏_target| > Œ∏_align$.
Acci√≥n:
- `v = 0`
- `œâ` seg√∫n ley proporcional a $Œ∏_target$
Transici√≥n:
- a AVANZAR cuando $|Œ∏_target| ‚â§ Œ∏_align$
- a BUSCAR si se pierde detecci√≥n por M frames

### Estado 2: AVANZAR
Condici√≥n: persona centrada ($|Œ∏_target| ‚â§ Œ∏_align$) y $A < A_target$.
Acci√≥n:
- `v > 0` (con reducci√≥n si no est√° perfectamente centrada)
- `œâ` peque√±o para mantener centrado
Transici√≥n:
- a DETENER si $A ‚â• A_target$
- a ALINEAR si $|Œ∏_target|$ crece (la persona se sale del centro)
- a BUSCAR si se pierde detecci√≥n

### Estado 3: DETENER
Condici√≥n: persona cerca ($A ‚â• A_target$).
Acci√≥n:
- `v = 0`, `œâ = 0`
Transici√≥n:
- a AVANZAR si $A < A_target$ (persona se aleja)
- a BUSCAR si no hay detecci√≥n

‚úÖ **Entregable adicional**: diagrama de estados (tipo bloque o UML simple).

---

## üß™ Pruebas obligatorias (m√≠nimo 4)
1. Persona aparece al frente ‚Üí el robot deja de buscar y **se alinea**.
2. Persona a la izquierda ‚Üí el robot gira hasta quedar centrado.
3. Persona a la derecha ‚Üí el robot gira hasta quedar centrado.
4. Persona al frente y lejos ‚Üí el robot avanza y se detiene a distancia segura.

‚ö†Ô∏è Seguridad:
- √Årea despejada
- L√≠mite de velocidad bajo
- Bot√≥n de paro o comando de emergencia listo

---

## üì¶ Entregables
1. üé• **Video (2‚Äì4 min)** mostrando:
   - BUSCAR ‚Üí detectar ‚Üí ALINEAR ‚Üí AVANZAR ‚Üí DETENER
   - Evidencia visual de segmentaci√≥n (m√°scara/caja)
2. üìÑ **Reporte t√©cnico (1‚Äì2 p√°ginas)** que incluya:
   - Par√°metros f√≠sicos `b` (y `r` si aplica)
   - Ecuaciones usadas (de `e` ‚Üí `Œ∏_target` ‚Üí `œâ` ‚Üí `v_L, v_R`)
   - Diagrama de m√°quina de estados
3. üìÅ **Repositorio** (o zip) con:
   - Documentaci√≥n de instalaci√≥n/ejecuci√≥n
   - Evidencias (capturas)
4. üß† **AI_LOG.md**
   - prompts usados
   - decisiones tomadas
   - problemas encontrados y c√≥mo los resolvieron

---

## üìä R√∫brica (100 pts)
| Criterio | Pts |
|---|---:|
| Segmentaci√≥n YOLOv8 de persona estable (con filtro de confianza) | 20 |
| C√°lculo correcto de error/√°ngulo (`e` ‚Üí `Œ∏_target`) y justificaci√≥n de FOV | 15 |
| Control cinem√°tico: `œâ` para alineaci√≥n + reglas de seguridad | 20 |
| Conversi√≥n cinem√°tica a ruedas/orugas (`v, œâ` ‚Üí `vL, vR`) + explicaci√≥n | 15 |
| M√°quina de estados robusta (buscar/alinear/avanzar/detener) | 10 |
| Pruebas en f√≠sico (4 casos) con evidencia clara | 10 |
| Reporte y documentaci√≥n reproducible | 5 |
| AI_LOG completo y reflexivo | 5 |

---

## ‚úÖ Checklist de aprobaci√≥n
- [ ] C√°mara ok y video estable
- [ ] YOLOv8-seg detecta ‚Äúperson‚Äù
- [ ] Se obtiene centroide y √°rea
- [ ] Se calcula `Œ∏_target` y se usa para orientar
- [ ] Se generan `v` y `œâ` con l√≠mites
- [ ] Se convierte a `vL` y `vR` mediante cinem√°tica
- [ ] M√°quina de estados implementada
- [ ] Video evidencia completo
